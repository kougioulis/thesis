\section{Future Directions} \label{sec:future_work}

Our work regarding LCMs results in many interesting avenues for further research. Approaches for expanding our models may arise from reducing the amount of causal assumptions needed for interpreting the output of our models. For instance, by relaxing the assumption of causal sufficiency, explicit modeling of latent confounders (e.g., indirect causes such as \( X^i_{t} \leftarrow X^k_{t-1} \rightarrow X^j_{t}\)) is enabled, while the same can be done for the handling of contemporaneous effects. Another interesting point of research is to the inclusion of direct causal effects in the output of our LCMs. As causal effects are available during training from the ground truth causal graph, it would be interesting to investigate estimating them using an auxiliary task within the proposed framework. With the rise of AI Agents fine-tuned to specific tasks, we envision the incorporation of the output of LCMs into agents for causal discovery \textit{(Causal AI Agents)}. Such agents would actively reason about interventions, simulate outcomes, and refine their understanding of the environment through causal experimentation. By embedding LCMs as internal causal world models, such agents could autonomously generate and test causal hypotheses, while their natural language could serve as a natural medium for \textit{AI Explainability}.

While our LCMs present advances in scalable temporal causal discovery, they remain primarily a graph-predicting model, without explicitly modeling or leveraging disentangled, causal representations of the underlying processes. Recent work by \citet{scholkopf2021towards} highlights the potential of learning modular and invariant causal representations as a foundation for robust and generalizable reasoning. An exciting direction for future research would therefore be extending our LCMs to jointly learn structured representations that encode the independent mechanisms and interventions of the system, potentially improving interpretability and zero-shot generalization.

By bridging causal discovery, representation learning, and agentic decision-making, such systems could ultimately form the backbone of \textit{causally grounded AI}, a new paradigm where models not only predict correlations but understand and manipulate the causes that generate them.