\section{Summary of Contributions}

Our first research avenue focused on establishing a comprehensive data generation pipeline for causal discovery. We developed a scalable framework for sampling synthetic random temporal structural causal models (TSCMs), enabling the automatic generation of corresponding samples via ancestral sampling. In contrast to existing approaches that are limited in both generation diversity and expressivity, our framework supports the creation of hundreds of thousands of SCM instances with high variability and controlled complexity.

The second avenue addressed the integration of realistic datasets into training, aiming to increase model expressivity and predictive robustness. We referred to these datasets as simulated. Motivated by evidence that the inclusion of realistic data improves foundation model performance in out-of-distribution scenarios, we proposed a novel method, \textit{Temporal Causal-based Simulation (TCS)}, to bridge the gap between purely synthetic and real-world data. TCS enables the generation of thousands of causal models grounded in empirical data, paired with a Min-Max optimization scheme, \textit{Adversarial Causal Tuning (ACT)}, that facilitates optimal causal model selection. Together with subsampling across time and variable dimensions, it significantly expanded the available training corpus. Additionally, it serves as a method for benchmarking causal discovery algorithms, which is of vital importance in the Causality community.

Building on this foundation, the use of sample statistics within training and model inference was explored, showing that statistical regularization improves learning stability and performance. The integration of synthetic training data with realistic data was shown to improve generalization and robustness, demonstrating that causal foundation models also benefit from a diverse training corpus. Additionally, the current state-of-the-art has been extended from five to twelve input variables. In contrast to prior work trained solely on synthetic data and prone to performance degradation under scaling, introduced models maintain stable and robust behavior across both in-distribution and out-of-distribution settings, while surpassing or performing on par with established causal discovery algorithms.

Finally, a novel neural architecture for LCMs was proposed, integrating mechanisms such as patch-based representations and alternating attention layers. Early experiments indicate promising behavior, although these findings remain preliminary in nature. Initial explorations into the incorporation of interventional data and prior knowledge suggest potential avenues for extending foundation models for causal discovery towards a more unified scope.

Overall, this work aims to establish the conceptual and technical frame for scalable, foundation-style models in temporal causal discovery. Through a combination of training data generation, design and pretraining strategies, the groundwork for neural causal discovery methods that are robust, generalizable and applicable to complex real-world systems has been paved.