\section{The Challenge} \label{sec:data-challenge}

The task of generating synthetic instances is evidently a more straightforward problem than the one of generating realistic ones. Various works that address synthetic causal data generation have been introduced, in both temporal and atemporal settings \citep{cinquini2021boosting, wen2021causal}, yet few assess the realism and quality of generated data. Concerning time-series, methods taking into account sophisticated and configurable generative frameworks for both data and the causal structure itself exist (such as \cite{lawrence2020cdml}). Methods such as Causal Chambers by \citet{gamella2024causal} for generating realistic time-series data from physical systems under controlled experimental conditions have also been recently introduced. A key difference between our approach and theirs is that our proposed pipeline remains fully algorithmic, enabling scalable and diverse data generation, instead of a closed and controlled experimental setup. The work of \citet{stein2024embracing} also includes a synthetic generator of time-series, as well as the Tigramite package by \cite{runge2019inferring}. A shortcoming of these works is that during ancestral sampling over the causal graph, cuasal effects may grow too large due to the functional relationships used, thus resulting in values outside floating point range. This issue is handled by re-instantiating the sampling process, which can prove time consuming and cumbersome for generating thousands of instances. What's more, some chosen functional relationships or parametric assumptions about the noise distributions may result in causal graphs that are either too trivial for the model to generalize or too hard for the model to extract any meaningful context. Furthermore, the work of \citet{stein2024embracing}, although it follows an additive time-invariant SCM formulation for samples, it directly proceeds by randomly populating the lagged adjacency tensor, which remains of questionable soundness, as no TSCM is directly introduced. Finally, graphs may prove to be too dense (causal graphs should be, in general, sparse), which may lead the model to either overfit or underfit the data. We take into consideration the above points for the introduction of our own synthetic generation pipeline to output coherent time-lagged causal graphs for training and evaluation of our LCMs.

In the case of real-world data, the challenge is much more complex. The main argument against the sole use of synthetic dta is that reliance on data generated under idealized assumptions, e.g., additive noise models, pre-defined structural priors, or linear interactions, limits data quality and as such the generalization capabilities of our LCMs (i.e. Figure \ref{fig:stein_fmri} in Chapter \ref{chap:introduction}). As we elaborate in detail in Section \ref{sec:data-simulated}, common evaluation protocols for realistic data generators use single-metric comparisons \citep{cheng2024causaltime}, which do not fully capture the generative realism or causal fidelity. What's more, assumptions of both purely synthetic and semi-synthetic causally generated data are unlikely to hold in practice, as real-world models are subject to inherent restrictions and do not fit under closed conditions assumed by such methods. Generating datasets that are both causally coherent and statistically realistic remains a core challenge addressed in this work. We call such datasets \textit{simulated} data.

The assumption (stemming from adopting a supervised learning approach as elaborated in Section \ref{sec:problem-formulation}), that \textit{distributions of training and test data cases must highly overlap}, poses another fundamental challenge in data generation. As one may expect, a supervised classification model (not limited to a neural network) trained on linear instances is expected to perform poorly on non-linear cases. This simple observations further highlights the need for \textit{curating a large data collection with various complexities, sizes and causal properties for training and evaluation}. For synthetic cases, this includes a wide family of graph sizes, node degrees, functional relationships and noise distributions, among others. 

To extend the limited dimensionality from five (\(5\)) variables of \citet{stein2024embracing} to more than double (twelve (\(12\))), the need for constructing an optimal curation of both synthetic and simulated data remains even more significant. The challenge of constructing a vast collection of data (among other number of variables, lags of causal relationships, functional dependencies etc.) is evident in order to avoid degradation of model performance as input dimensionality and model parameters increase (Figure \ref{fig:stein_auc} in Chapter \ref{chap:introduction}).

Finally, one should be accounting for causal assumptions that are not only reflected in the training data but also in the assumptions of the model. For instance, consider generating observational samples from a causal structure \(X_{t} \leftarrow Z_{t-1} \rightarrow Y_{t}\) where \(Z\) is assumed to be a latent confounder and then removed from the corresponding SCM. A model that assumes causal sufficiency would potentially infer (based on these observational samples) a directed edge between \(X_t\) and \(Y_{t}\) (of any lag), which clearly is not the case and results in false interpretation of discovered causal relationships.