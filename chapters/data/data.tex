\chapter{Data Generation} \label{chap:data}

\begin{chapquote}{\textit{Abraham Lincoln}}
``If I had six hours to chop down a tree, I'd spend the first four sharpening the axe.''
\end{chapquote}

A central limitation of current causal discovery methods, either in the temporal or i.i.d. setting, is the exclusive reliance on synthetic datasets for their evaluation, benchmarking and recently, for foundational-inspired methods (Section \ref{sec:related-work}), training. This chapter outlines our methodologies for generating vast data instances\footnote{We refer to a pair of a multivariate time-series and its corresponding causal graph as a \textit{data instance} or simply dataset, when no ambiguity exists regarding the form of data. Multiple pairs of data instances are referred to as a \textit{data collection.}} to support robust training of our large causal models. Synthetic data generation pipelines are typically constructed using predefined causal structures and known functional dependencies from randomly generated SCMs. While such approaches offer precise control and clarity over the data generation process, they often fail to capture the complexity and statistical idiosyncrasies of real-world time-series. To address this, we propose generation methods of both \textit{synthetic} and \textit{simulated} (realistically generated) data. Briefly, we opt for training on a mixture of both synthetic and simulated datasets. 

We begin with an introduction to the context of data generation (Section \ref{sec:data-introduction}) and proceed by outlining the main challenges for causal discovery (Section \ref{sec:data-challenge}). The remainder is structured as follows: Section \ref{sec:data-synthetic} outlines the generation process of synthetic data pairs, which serve as a learning foundation for our Large Causal Models. Section \ref{sec:data-simulated} introduces the \textit{Temporal Causal-based Simulation (TCS)} framework, a modular and model-agnostic generative method for causal models and corresponding time-series samples, as well as the \textit{Adversarial Causal Tuning (ACT)} module, which enables fine-tuning of TCS based on a Min-max optimization approach. Unlike traditional synthetic data generators in literature, our methodology learns the structural and functional dependencies, as well as the noise distribution, directly from time-series samples, and by allowing fine-tuning of the process further enhances the realism of generated causal models and bridges the domain gap between synthetic training and applicability to real-world scenarios. This part serves as a crucial component for enhancing the generalization performance of our LCMs, allowing us to leverage data of higher quality for not only training, but also evaluation and benchmarking. Section \ref{sec:data-training} covers the curation of training and validation pairs used for model training, as well as the construction of our benchmark datasets. Finally, Section \ref{subsec:shardification} provides an overview of our approach for partitioning data into shards for efficient training of LCMs.

\input{chapters/data/introduction}

\input{chapters/data/challenge}

\input{chapters/data/synthetic}

\input{chapters/data/simulated}

\input{chapters/data/interv}

\input{chapters/data/training}