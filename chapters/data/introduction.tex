\section{Introduction} \label{sec:data-introduction}

In the foundation model paradigm, model performance and generalization is closely tied to training data quality and diversity \citep{bommasani2021opportunities}. In the same way foundation models for natural language processing (e.g. Large Language Models-LLMs like the family of GPTs \citep{radford2019language, brown2020language, achiam2023gpt}) and computer vision (e.g. CLIP \citep{radford2021learning}, DALL-E \citep{ramesh2021zero}) rely on vast and heterogeneous data collections to fully capture the complexities of the real world in order to generalize effectively, a foundation model for causal discovery would require training data that reflect the complexities and properties of real-world causal systems.

For the most part, obtaining real data-cases is a straightforward task for domains unrelated to causality. Consider for example the case of constructing a neural model for recognizing images (such as dogs, cats, a glass bottle etc.). Real-world images are widely available online and can even be collected easily nowadays. For a supervised learning like image classification, it is then a matter of \textit{annotating} each image with its ground truth label. Such labeled image collections have already been curated, such as Imagenet \citep{deng2009imagenet}. A similar scenario exists for time-series forecasting models as well \citep{hahn2023time, kim2025comprehensive}. Foundation models for time-series forecasting have been developed and trained on such data collections, such as the Transformer XL \citep{dai2019transformer}, the Informer \citep{zhou2021informer} etc.

An ongoing challenge in causal discovery is obtaining real data-cases, as it represents a much more nuanced task that generating purely synthetic ones. In the absence of real-world data and causal graph pairs, existing works regarding causal discovery on both \textit{iid} and temporal data tend to focus on using either on (i) \textit{purely synthetic} samples (causal models with randomized Erd\H{o}s-Renyi \citep{brouillard2020differentiable} or Barab\'{a}si-Albert \citep{barabasi1999emergence} structures) or (ii) \textit{semi-synthetic}, based on existing knowledge of real cases and scenarios (e.g., simulated Markov processes of physical systems). Examples of such datasets include the Kuramoto model of coupled oscillators \citep{kuramoto1975self, lowe2022amortized} and brain connectivity models such as \(f\)MRI \citep{smith2011network}. Observational data from synthetic causal models are then generated through \textit{ancestral sampling} \citep[Section 4.2.5]{murphy2023probabilistic} from the specified temporal SCM, and interventional data, if needed, in a similar manner. While this approach provides full control over the form of the causal structure and resulting causations, it leads to inherent limitations: synthetic data generation operates under idealized assumptions, including fully specified models, simplified noise distributions, absence of latent confounding, noisy interactions etc. As a result, generated data often fail to capture the complexity, heterogeneity, and non-stationary characteristics of real-world time-series. Real time-series samples are vastly available, but each one must be accompanied with its ground truth causal mechanism, from which data samples are generated from. As many causal assumptions on which causal discovery and causal inference methods are built upon (as elaborated in Chapter \ref{chap:problem-formulation}) are unknown or unlikely to hold in such real-world systems, the need for realistic data cases is even more pressing.

Regarding training of foundation models, the authors of TimesFM \citep{das2024decoder} articulate this principle clearly in the context of time-series forecasting: \\

\begin{chapquote}{\textit{Rajat Sen and Yichen Zhou}, TimesFM authors\footnote{\url{https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting} (Last Accessed: May 2025).}}
``Synthetic data helps with the basics... Real-world data adds real-world flavor... This helps TimesFM understand the bigger picture and generalize better when provided with domain-specific contexts not seen during training.''
\end{chapquote}

This sharp observation holds equally true for foundation models for causal discovery: synthetic data encode fundamental structural and temporal dependencies, while realistic data capture the statistical irregularities and intractable functional complexities inherent in real-world systems.

Surprisingly, causal discovery methods, including recent foundation models presented in Section \ref{sec:related-work} such as CP \citep{stein2024embracing}, SEA \citep{wu2024sample} and Do-PFN \citep{robertson2025do} are trained exclusively on synthetic datasets. Such models are rarely exposed to the structural uncertainty, noise variability or irregular patterns present in real data. Furthermore, their evaluation is typically restricted to synthetic benchmarks, limiting their demonstrated generalization capacity in practical deployment scenarios.

To address the above concerns, our work adopts a dual-strategy approach for data generation. Alongside a conventional yet rigid synthetic data generation pipeline from temporal SCMs, we introduce realistically generated datasets constructed using Adversarial Causal Tuning - ACT, inspired by AutoML \citep{hutter2019automated}. In short, the method generates time-series and corresponding lagged causal graphs by learning directly from real data samples. It decomposes the data generation process into three modular phases: (i) discovery of the causal graph, (ii) approximation of the functional dependencies and (iii) noise distribution modeling, all under a Min-max fine tuning scheme for optimal causal model selection. In this way, we allow generation of time-series that are both causally coherent and statistically realistic.

%To address the above concerns, our work adopts a dual-strategy approach for data generation. Alongside a conventional yet rigid synthetic data generation pipeline from temporal SCMs, we introduce realistically generated datasets constructed using the \textit{Temporal Causal-based Simulation} (TCS) framework. TCS generates time-series and corresponding lagged causal graphs by learning directly from real data samples. In short, it decomposes the data generation process into three modular phases: (i) discovery of the causal graph, (ii) approximation of the functional dependencies and (iii) noise distribution modeling, all under a Min-max fine tuning scheme for optimal causal model selection (\textit{Adversarial Causal Tuning - ACT}) inspired by AutoML \citep{hutter2019automated}. In this way, we allow generation of time-series that are both causally coherent and statistically realistic.