\section{Training Protocol and Data Splits} \label{sec:training-prot}

All models are trained on the curated datasets described in Chapter \ref{chap:data}. Each dataset follows an 80/10/10\% split into training, validation, and test subsets to ensure a fair evaluation and prevent data leakage. Mini-batches are randomly sampled from the training set and optimization is performed using the AdamW optimizer (Section \ref{sec:optimizer}) with default parameters \(\beta_1 = 0.9\), \(\beta_2 = 0.999\) for a maximum of \(100\) epochs.